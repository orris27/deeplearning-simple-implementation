{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "\n",
    "\n",
    "1. A loss function to be optimized\n",
    "2. A weak learner to make decisions\n",
    "3. An additive model to add weak learners to minimize the loss function\n",
    "\n",
    "## Ensemble\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*8T4HEjzHto_V8PrEFLkd9A.png)\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*PaXJ8HCYE9r2MgiZ32TQ2A.png)\n",
    "### 1. Bagging\n",
    "Bagging is an ensemble technique in which we build many independent predictors and combine their decisions using some averaging techniques.\n",
    "\n",
    "\n",
    "### 2. Boosting\n",
    "Boosting an ensemble technique in which the predictors are not made independently, but sequentially. The subsequent predictors learn from the mistakes of the previous predictors. Gradient Boosting is an example of the boosting algorithm.\n",
    "\n",
    "\n",
    "\n",
    "## Weak Learner\n",
    "Muiltiple ways of restricting the decision tree to be weak learner:\n",
    "+ number of leaf nodes\n",
    "+ tree depth\n",
    "\n",
    "## Gradient Descent\n",
    "Instead of updating weights in neural networks or coefficients in a regression equation, gradient boosting adds a new weak learner to reduce the loss.\n",
    "\n",
    "## Algorithm\n",
    "Regression Problem\n",
    "1. Average the label values and use it as the initial leaf for task\n",
    "2. Calculate the residuals between the correct values and the predicted values, and use them as leaves for new tree.\n",
    "3. Sort the residuals and merge adjacent leaves to ensure the number of leaves are within the allowed number\n",
    "4. Build a new decision tree whose leaves are the residul.\n",
    "5. Scale the new decision tree with a factor, such as 0.1, and this tree will participate in the computation of the prediction\n",
    "6. repeat step 2-5 until the errors drop to the given value.\n",
    "\n",
    "## Questions\n",
    "1. What is the parameter of a tree?\n",
    "2. How to build a new decision tree while keep the leaf nodes in order?\n",
    "3. Is it possible that we can always find a weak tree that allows a sample to be classified to a specific leaf node?\n",
    "\n",
    "## References\n",
    "+ [Gradient Boosting from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
