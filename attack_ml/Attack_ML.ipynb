{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FGSM, Basic Iterative Method, L_BFGS, Deepfool, JSMA, C&W, Elastic net attack, Spatially Transformmed, One Pixel Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property\n",
    "\n",
    "+ An adversarial example trained with one architecture on one dataset can fool models that utilize different architectures on different datasets.\n",
    "+ If we add multiple noise to an adversarial/clean image, the image stays adversarial/clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q & A\n",
    "### Why adversarial examples can fool well-trained model?\n",
    "Adversarial exmaples have different distributions from those of input images. For instance, an adversarial example can have value between $(\\frac{1}{255}, \\frac{2}{255})$, while a real image does not have such pixel.\n",
    "\n",
    "### If we add noise to the input tensor, can the trained model be resistant to adversarial examples?\n",
    "+ Noise can produce images that are not in standard distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Approaches\n",
    "\n",
    "### L-BFGS\n",
    "[Intriguing properties of neural networks](https://arxiv.org/pdf/1312.6199.pdf)\n",
    "\n",
    "### FGSM\n",
    "\n",
    "[Expaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572.pdf)\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Fix the model's parameters, while make the pixels in input image differentiable. Forward the input image in the model and we will get the gradients for each pixel.\n",
    "2. Update the image with the following formula:\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{x} + \\epsilon \\text{sign}(\\bigtriangledown_{\\mathbf{x}}{J(\\theta, \\mathbf{x}, y)}),\n",
    "$$\n",
    "where $\\epsilon$ is a small constant and $J(\\theta, \\mathbf{x}, y)$ is the loss function same as what we use in training a classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
