{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FGSM, Basic Iterative Method, L_BFGS, Deepfool, JSMA, C&W, Elastic net attack, Spatially Transformmed, One Pixel Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property\n",
    "\n",
    "+ An adversarial example trained with one architecture on one dataset can fool models that utilize different architectures on different datasets.\n",
    "+ If we add multiple noise to an adversarial/clean image, the image stays adversarial/clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q & A\n",
    "### Why adversarial examples can fool well-trained model?\n",
    "Adversarial exmaples have different distributions from those of input images. For instance, an adversarial example can have value between $(\\frac{1}{255}, \\frac{2}{255})$, while a real image does not have such pixel.\n",
    "\n",
    "### If we add noise to the input tensor, can the trained model be resistant to adversarial examples?\n",
    "+ Noise can produce images that are not in standard distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Approaches\n",
    "\n",
    "### 1. L-BFGS\n",
    "[Intriguing properties of neural networks](https://arxiv.org/pdf/1312.6199.pdf)\n",
    "\n",
    "### 2. FGSM\n",
    "\n",
    "[Expaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572.pdf)\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Fix the model's parameters, while make the pixels in input image differentiable. Forward the input image in the model and we will get the gradients for each pixel.\n",
    "2. Update the image with the following formula:\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{X} + \\epsilon \\text{sign}(\\bigtriangledown_{\\mathbf{X}}{J(\\theta, \\mathbf{X}, y_{true})}),\n",
    "$$\n",
    "where $\\epsilon$ is a small constant (such as $0.007$) and $J(\\theta, \\mathbf{X}, y_{true})$ is the loss function same as what we use in training a classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DeepFool\n",
    "[DeepFool notebook](deepfool.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Jacobian-based Saliency Map Attack (JSMA)\n",
    "[The Limitations of Deep Learning in Adversarial Settings](https://arxiv.org/abs/1511.07528)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Basic Iterative Method\n",
    "[Adversarial Machine Learning at Scale](https://arxiv.org/pdf/1611.01236)\n",
    "\n",
    "A straightforward extension of FGSM is to apply it multiple times with small step size:\n",
    "$$\n",
    "\\mathbf{X_0} = \\mathbf{X} \\\\\n",
    "\\mathbf{X_{i+1}} = \\text{clip}_{X, \\epsilon}(\\mathbf{X_i} + \\alpha \\text{sign}(\\bigtriangledown_{\\mathbf{X_i}}{J(\\theta, \\mathbf{X_i}, y_{true})})),\n",
    "$$\n",
    "where the author uses $\\alpha=1$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
