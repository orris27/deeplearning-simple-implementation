{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Features of transformer:\n",
    "+ bidirectional RNN (takes a sequence as input and outputs a new sequence)\n",
    "+ friendly to parallism\n",
    "\n",
    "\n",
    "To learn more:\n",
    "+ paper: [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "+ video: [Transformer](https://www.youtube.com/watch?v=ugWDIIOHtPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "### 1. Scaled dot-product attention\n",
    "The input vectors are called $X = \\begin{bmatrix}x^1 & x^2 & \\cdots & x^T\\end{bmatrix}$, where $x^t$ is the input vector at $t$th timestamp and $T$ is the total timestamps. \n",
    "\n",
    "1. Firstly, $X$ is fed into embedding layer and outputs $A = \\begin{bmatrix}a^1 & a^2 & \\cdots & a^T\\end{bmatrix}$. \n",
    "\n",
    "2. Then, we multiply $A$ with $3$ matrices, namely $W^q$, $W^k$ and $V^t$, to obtain $3$ matrices, namely $Q = \\begin{bmatrix}q^1 & q^2 & \\cdots & q^T\\end{bmatrix}$, $K = \\begin{bmatrix}k^1 & k^2 & \\cdots & k^T\\end{bmatrix}$ and $V = \\begin{bmatrix}v^1 & v^2 & \\cdots & v^T\\end{bmatrix}$. \n",
    "\n",
    "3. We pair each $k^i$ with each $q^j$ to get the attentions, namely $\\text{Attention} = \\begin{bmatrix}\\alpha_{11} & \\alpha_{12} & \\cdots & \\alpha_{1T} \\\\ \\alpha_{21} & \\alpha_{22} & \\cdots & \\alpha_{2T} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\alpha_{T1} & \\alpha_{T2} & \\cdots & \\alpha_{TT}\\end{bmatrix}$, where $\\alpha_{ij}$ is the inner product of $k^i$ and $q^j$ divided by $\\sqrt{d_k}$ and $d_k$ is the dimension of $k^i$ or $q^j$ (their dimensions are the same), i.e., $\\alpha_{ij} = \\frac{k^i \\odot q^j}{\\sqrt{d_k}}$.\n",
    "\n",
    "4. We apply column-wise softmax layer to $\\text{Attention}$ to obtain $\\hat{\\text{Attention}} = \\begin{bmatrix}\\hat{\\alpha_{11}} & \\hat{\\alpha_{12}} & \\cdots & \\hat{\\alpha_{1T}} \\\\ \\hat{\\alpha_{21}} & \\hat{\\alpha_{22}} & \\cdots & \\hat{\\alpha_{2T}} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat{\\alpha_{T1}} & \\hat{\\alpha_{T2}} & \\cdots & \\hat{\\alpha_{TT}}\\end{bmatrix}$\n",
    "\n",
    "5. Finally, we perform matrix mulitplication between $V$ and $\\hat{\\text{Attention}}$ to obtain the output $O$, i.e., $O = V \\times \\hat{\\text{Attention}}$\n",
    "\n",
    "\n",
    "The illustration of transformer is shown in the following figure. In this figure, the input $I$ is the aforementioned $A$.\n",
    "<img src=\"images/algorithm.png\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-head attention\n",
    "Different heads focus on different features, for example, one head pays attention to local information, while the other focuses on global information.\n",
    "\n",
    "We further transform $Q$, $K$ and $V$ into several parts and perform scaled dot-product attention for each part individually to get a list of outputs. Concat this outputs to form the final outputs or perform additional transform before obtaining the final outputs.\n",
    "<img src=\"images/multi_head_formula.png\" width=\"350\"/>\n",
    "\n",
    "<img src=\"images/multi_head1.png\" width=\"350\"/>\n",
    "<img src=\"images/multi_head2.png\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "The predescribed algorithm does not take positions into consideration, positional encoding is added to provide position information. The positional encoding is defined as the function below and it is directly summed with the input embeddings, and thus positional encoding must have the same dimension as $A$.\n",
    "\n",
    "$$\n",
    "PE({}_{pos}, {}_{2i}) = sin(pos / 10000^{2i / d_{model}}) \\\\\n",
    "PE({}_{pos}, {}_{2i+1}) = cos(pos / 10000^{2i / d_{model}}),\n",
    "$$\n",
    "where $pos$ is the position and $i$ is the dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add & Norm\n",
    "Sum multi-head outputs with inputs and perform [layer normalization](https://arxiv.org/abs/1607.06450) which is used in RNN.\n",
    "\n",
    "Layer normalization hopes data in the same dimension follows a standard normal distribution.\n",
    "<img src=\"images/layer_normalization.png\" width=\"200\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
