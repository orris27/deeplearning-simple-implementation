{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newsgroups\n",
    "## 1. word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_names: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "training_dataset size: 11314\n",
      "count vector shape: (11314, 130107)\n",
      "x_train_counts.shape: (11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "print('target_names:', twenty_train.target_names)\n",
    "print('training_dataset size:', len(twenty_train.data))\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "# See the contents\n",
    "# for article in twenty_train.data[:3]:\n",
    "#     myindex = twenty_train.data.index(article)\n",
    "#     print(\"\\n*** Article #{} Label: {} ***\\n\\n\".format(myindex, \\\n",
    "#                                                        twenty_train.target_names[twenty_train.target[myindex]]))\n",
    "#     print(article)\n",
    "    \n",
    "count_vect = CountVectorizer()\n",
    "x_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "print('count vector shape:', x_train_counts.shape)\n",
    "y_train_counts = twenty_train.target\n",
    "# print('x_train_counts[0]', x_train_counts[0])\n",
    "\n",
    "x_test_counts = count_vect.transform(twenty_test.data) # get counts on the words in twenty_test data\n",
    "print('x_train_counts.shape:', x_train_counts.shape)\n",
    "y_test_counts = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 6 1 2 1 1 1 1 1 2 2 1 4 1 1 1 1 1 3 1 1 1 1\n",
      " 1 1 5 3 5 1 1 1 1 2 2 2 2 2 3]\n",
      "[ 0 89]\n",
      "[ 86580 128420  35983  35187  66098 114428  78955  94362  76722  57308\n",
      "  62221 128402  67156 123989  90252  63363  78784  96144 128026 109271\n",
      "  51730  86001  83256 113986  37565  73201  27436  34181 101378 106116\n",
      "  35612  56989  26073  66608 108252  99822 123796  48620  34995  37433\n",
      "  18299  50111  16574  74693  32311 115475  76718 109581  48618  68766\n",
      "  45295  90686 114455 104813  89860  80638  51793  42876 114579  90774\n",
      "  28615  65798 124931 123292   4605  76032  92081  40998  79666  89362\n",
      " 118983  90379  98949  64095  95162  87620 114731  68532  37780 123984\n",
      " 111322 114688  85354 124031  50527 118280 123162  75358  56979]\n",
      "from\n"
     ]
    }
   ],
   "source": [
    "# X_train_counts[0].data – \n",
    "# X_train_counts[0].indptr – Tells us where the article starts and ends. In our case this\n",
    "# would be 0 and 89, because there are 89 words in this article.\n",
    "# X_train_counts[0].indices – Tells us which words were actually used.\n",
    "\n",
    "A = x_train_counts[0]\n",
    "print(A.data) # The actual word count\n",
    "print(A.indptr) # Tells us where the article starts and ends. In our case this\n",
    "# would be 0 and 89, because there are 89 words in this article.\n",
    "print(A.indices[A.indptr[0]:A.indptr[1]]) # Tells us which words were actually used.\n",
    "\n",
    "# for index in A.indices[A.indptr[0]:A.indptr[1]]:\n",
    "#     print(count_vect.get_feature_names()[index]) # a list of integer\n",
    "    \n",
    "print(count_vect.get_feature_names()[A.indices[88]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn.fit(x_train_counts, y_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7728359001593202\n"
     ]
    }
   ],
   "source": [
    "y_predicted = mn.predict(x_test_counts)\n",
    "score = accuracy_score(y_predicted, y_test_counts)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_names: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "training_dataset size: 11314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orris/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "print('target_names:', twenty_train.target_names)\n",
    "print('training_dataset size:', len(twenty_train.data))\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts) # we use counts to calculate their tfidf\n",
    "y_train_tfidf = twenty_train.target\n",
    "\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n",
    "y_test_tfidf = twenty_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_tfidf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_tfidf.fit(x_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7738980350504514\n"
     ]
    }
   ],
   "source": [
    "y_predicted = mn_tfidf.predict(x_test_tfidf)\n",
    "score = accuracy_score(y_predicted, y_test_tfidf)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF without stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orris/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "count_vect_nostopwords = CountVectorizer(stop_words='english')\n",
    "\n",
    "x_train_counts_nostopwords = count_vect_nostopwords.fit_transform(twenty_train.data)\n",
    "x_test_counts_nostopwords = count_vect_nostopwords.transform(twenty_test.data) # get counts on the words in twenty_test data\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_tfidf_nostopwords = tfidf_transformer.fit_transform(x_train_counts_nostopwords) # we use counts to calculate their tfidf\n",
    "y_train_tfidf_nostopwords = twenty_train.target\n",
    "\n",
    "x_test_tfidf_nostopwords = tfidf_transformer.transform(x_test_counts_nostopwords)\n",
    "y_test_tfidf_nostopwords = twenty_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_tfidf_nostopwords = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn_tfidf_nostopwords.fit(x_train_tfidf_nostopwords, y_train_tfidf_nostopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8169144981412639\n"
     ]
    }
   ],
   "source": [
    "y_predicted = mn_tfidf_nostopwords.predict(x_test_tfidf_nostopwords)\n",
    "score = accuracy_score(y_predicted, y_test_tfidf_nostopwords)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8224907063197026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=5, random_state=42)\n",
    "svm.fit(x_train_tfidf_nostopwords, y_train_tfidf_nostopwords)\n",
    "y_predicted = svm.predict(x_test_tfidf_nostopwords)\n",
    "score = accuracy_score(y_predicted, y_test_tfidf_nostopwords)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
